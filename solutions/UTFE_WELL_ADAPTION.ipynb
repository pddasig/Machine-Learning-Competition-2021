{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning Based Type Well Selection and Its Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __Author__: Wen Pan, Tianqi Deng\n",
    "* __Date__: Feb 1st, 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <a>1. Introduction </a>\n",
    "    - <a>1.1. Summary and Disclaimer</a>\n",
    "    - <a>1.2. Problem Statement</a>\n",
    "    - <a>1.3. Data Description</a>\n",
    "    - <a>1.4. Evaluation Metric</a>\n",
    "- <a>2. Imports</a>\n",
    "- <a>3. Read in Data</a>\n",
    "- <a>4. Glimpse of Data</a>\n",
    "- <a>5. Exploratory Data Analysis</a>\n",
    "- <a>6. Type Well Selection and Model Building</a>\n",
    "- <a>7. Discussion</a>\n",
    "\n",
    "<a></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we mainly demonstrate methods for statistics/ML based type well selection and how to integrate these methods with simple machine learning models to perform well-log interpretation. Please refer to **\"Reducing the Uncertainty of Multi-Well Petrophysical Interpretation from Well Logs with Statistical and Machine Learning Models\" by Pan et al, submitted, 2022.**, **\"Feature Engineering in Well-Log Interpretation\" presented by Pan et al. at GEOGULF 2021** and **\"Well-Log Normalization and Well-to-Well Correlation via Machine Learning Methods\" presented by Pan et al. at 2021 UT DIRECT and Formation Evaluation Consortium** for more details about assumptions and more advanced workflows.\n",
    "\n",
    "We did some DOE to find the optimal choice of methods, below are our findings based on our multiple submissions:\n",
    "\n",
    "- We find by incorporating well adaption proposed by Pan et al. (2022), we successfully reduce the uncertainty associated with multi-well well-log interpretation. \n",
    "\n",
    "- The adaption method was proven to be useful in well-log interpretation for clean carbonate reservoir, and in this conpetition we demonstrate that this method can also be used for shaly reservoir.\n",
    "\n",
    "- Compared to last year, this year we performed automatic well correlation instead of mannual correlation to help identify type well, greatly reducing the time for preprocessing.\n",
    "\n",
    "- The application of well adaption mitigates errors and non-uniqueness introduced by different borehole environments and allow more robust interpretation as discussed in Pan et al(2022).\n",
    "\n",
    "- semisupervised learning integrated with spatial continuity help mitigate the error introduced by imputing data with bad coverage.\n",
    "\n",
    "- Since we mostly use the simplist ML model in this workflow, only a few hyperparameters are tuned.\n",
    "\n",
    "- We strongly recommend interested readers to read \"Reducing the Uncertainty of Multi-Well Petrophysical Interpretation from Well Logs with Statistical and Machine Learning Models\" to better understand the assumptions and reasoning of this method, and how this method can be incorporated with other constraints and machine learning models to help improve well-log interpretation accuracy. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Please cite following publications if you are using this workflow for academic study.\n",
    "- \"Reducing the Uncertainty of Multi-Well Petrophysical Interpretation from Well Logs with Statistical and Machine Learning Models\", Pan et al.(2022)\n",
    "- \"Feature Engineering in Well-Log Interpretation\" presented by Pan et al. at GEOGULF 2021 \n",
    "- T. Giorgino. Computing and Visualizing Dynamic Time Warping Alignments in R: The dtw Package.\n",
    "- J. Stat. Soft., doi:10.18637/jss.v031.i07\n",
    "- Pérez-Cruz, F. Kullback-Leibler divergence estimation ofcontinuous distributions IEEE International Symposium on Information Theory, 2008."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a>1.2. Problem Statement</a>\n",
    "The objective of this contest is to develop data-driven models to estimate reservoir properties, including shale volume, porosity, and fluid saturation, based on a common set of well logs, including gamma ray, bulk density, neutron porosity, resistivity, and sonic. The participants will be provided with log data from eight wells from the same field together with the corresponding reservoir properties estimated by petrophysicists. They need to build a data-driven model using the provided training data set. Following that, they will deploy the newly developed data-driven models on the test data set to predict the reservoir properties based on\n",
    "the well-log data.\n",
    "\n",
    "You will be provided with log data from about 10 wells from the same field together with the corresponding reservoir properties estimated by petrophysicists. You need to build a data-driven model using the provided training dataset. Following that, you will deploy the newly developed data-driven models on the test dataset to predict the reservoir properties based on the well log data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a>1.3. Data Description</a>\n",
    "#### Files\n",
    "&gt; #### train.csv\n",
    "(All the values equals to -9999 are marked as missing values.)\n",
    "- WELLNUM - Well number\n",
    "- DEPTH - Depth, unit in feet\n",
    "- DTC - Compressional Travel-time, unit in nanosecond per foot\n",
    "- DTS - Shear Travel-time, unit in microseconds per foot\n",
    "- BS - Bit size, unit in inch\n",
    "- CAL - Caliper, unit in Inc\n",
    "- DEN - Density, unit in Gram per cubic centimeter \n",
    "- DENC - Corrected density, unit in Gram per cubic centimeter \n",
    "- GR - Gamma Ray, unit in API\n",
    "- NEU - Neutron, unit in dec\n",
    "- PEF - Photo-electric Factor, unit in barns/e\n",
    "- RDEP - Deep Resistivity, unit in Ohm.m\n",
    "- RMED - Medium Resistivity, unit in Ohm.m\n",
    "- ROP - Rate of penetration, unit in meters per hour\n",
    "- PHIF - Porosity, a unit equals to the percentage of pore space in a unit volume of rock.\n",
    "- SW - Water saturation\n",
    "- VSH - Shale Volume\n",
    "\n",
    "&gt; #### test.csv\n",
    "The test data has all features that you used in the train dataset, except PHIF, SW, and VSH.\n",
    "\n",
    "&gt; ####  sample_submission.csv\n",
    "A valid sample submission.\n",
    "<p><font style=\"\">\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a>1.4. Evaluation Metric</a>\n",
    "Submissions are evaluated according to root mean squared error(RMSE)\n",
    "\n",
    "$$RMSE = \\sqrt{\\frac{1}{m}\\sum_{i=1}^{m}(\\hat{\\mathbf{y_{i}}} - \\mathbf{y_{i}})^{2}} = \\sqrt{\\frac{1}{m}\\sum_{i=1}^{m}(\\hat{y}_{1, i} - y_{1, i})^{2}+(\\hat{y}_{2, i} - y_{2, i})^{2}+(\\hat{y}_{3, i} - y_{3, i})^{2}}$$\n",
    "\n",
    "where\n",
    "- $\\hat{y_i}$ is the predicted values of the true values $y_i$. Both $\\hat{y_i}$ and $y_i$ are vectors with 3 elements: $y_{1,i}$ - PHIF, $y_{2,i}$ - SW, and $y_{3,i}$ - VSH. \n",
    "- $m$ is sample size.\n",
    "\n",
    "**Note**:\n",
    "- Please remember to use random_state for all randomization steps, so the results are reproducible. \n",
    "- PHIF, SW, and VSH are in the same weight during the evaluation. \n",
    "- Understanding and optimizing your predictions for this evaluation metric is paramount for this competition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a>2. Imports</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"ticks\")\n",
    "from scipy.ndimage import median_filter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestRegressor, IsolationForest\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy.stats import mode\n",
    "from scipy.spatial import cKDTree as KDTree\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from scipy.spatial import KDTree\n",
    "# from dtw import *\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "np.random.seed(666)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a>3. Read Data</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-d0dabd83f776>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcol_names\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0;34m[\u001b[0m\u001b[0;34m'WELLNUM'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'DEN'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'GR'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'NEU'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcol_names2\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0;34m[\u001b[0m\u001b[0;34m'WELLNUM'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'DTC'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'DEN'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'GR'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'NEU'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'log_rd'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Replace -9999 with np.nan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1048\u001b[0m             )\n\u001b[1;32m   1049\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1866\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1867\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1868\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1869\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \"\"\"\n\u001b[0;32m-> 1362\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"replace\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train.csv'"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv('train.csv')\n",
    "col_names =  ['WELLNUM','DEN', 'GR', 'NEU'] + list(df1.columns.values[-3:])\n",
    "col_names2 =  ['WELLNUM','DTC','DEN', 'GR', 'NEU','log_rd']\n",
    "\n",
    "# Replace -9999 with np.nan\n",
    "df1.replace(['-9999', -9999], np.nan, inplace=True)\n",
    "df1['log_rd']=np.log(df1['RDEP'])\n",
    "\n",
    "# per well property\n",
    "wells=df1['WELLNUM'].unique()\n",
    "per_well=[df1[df1['WELLNUM']==i].copy() for i in wells] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a>4. Glimpse of Data</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check well log size for potential imbalanced well data\n",
    "log_size=[i.shape[0] for i in per_well]\n",
    "print(log_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "df1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import test data and do simple imputation, iteration will be performed to imrpove the imputation results\n",
    "df2 = pd.read_csv('test.csv')\n",
    "df2.replace(['-9999', -9999], np.nan, inplace=True)\n",
    "\n",
    "wells2=df2['WELLNUM'].unique()\n",
    "per_well2=[df2[df2['WELLNUM']==i].copy() for i in wells2]   \n",
    "\n",
    "# impute data with KNN\n",
    "for i in per_well2:\n",
    "    i['log_rd']=np.log(i['RDEP'])\n",
    "    imp = KNNImputer(n_neighbors=1)\n",
    "    i.loc[:,col_names2[1:]]=imp.fit_transform(i.loc[:,col_names2[1:]].values)\n",
    "    i['RDEP']=np.exp(i['log_rd'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a>5. EDA</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploratory Data Analysis (EDA) is an open-ended process where we calculate statistics and make figures to find trends, missing values, outliers, anomalies, patterns, or relationships within the data. \n",
    "\n",
    "- We examined summary statistics, pair-wise statistics and data coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.keys() # available logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# visualize gamma ray of the training and test set, the training data has a larger coverage compared to test data.\n",
    "for i in per_well:\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.plot(i['DEPTH'],i['GR'])\n",
    "    plt.ylim([0,150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# visualize test data, smaller coverage of depth\n",
    "for i in per_well2:\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.plot(i['DEPTH'],i['GR'])\n",
    "    plt.ylim([0,150])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a>6. Type Well Selection and Model Building</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation\n",
    "\n",
    "From the scatter plot between VSH and GR we found a strong linear correlation, other features are excluded from VSH prediction to avoid collinearlity. However, the linear trend varies with well, therefore we decide to use methods proposed by Pan et al.(2021), to adapt test wells to training wells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different linear correlation in different wells, we assume rock is a mixture of pure sand and pure shale, \n",
    "# and the GR of pure sand and shale can uniquely decide the model.\n",
    "# VSH<0 and VSH>1 represent pure sand and shale respectively\n",
    "plt.figure(figsize=(15,5))\n",
    "sns.scatterplot(data=df1,x='GR',y='VSH',hue='WELLNUM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize training and test data converage in the feature space\n",
    "for i in per_well:\n",
    "    i['type']='train'\n",
    "for i in per_well2:\n",
    "    i['type']='test'\n",
    "all_well=per_well+per_well2\n",
    "all_dat=pd.concat(all_well,axis=0)\n",
    "all_dat=all_dat.dropna(axis=0, subset=['GR','DEN','NEU','log_rd'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From scatter plot we can find a good coverage of test data\n",
    "sns.pairplot(all_dat[[ 'GR', 'DEN', 'NEU', 'log_rd', 'PHIF', 'SW', 'VSH','type']].sample(2000), hue='type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(all_dat.loc[all_dat['type']=='train',[ 'GR', 'DEN', 'NEU', 'log_rd', 'PHIF', 'SW', 'VSH',\n",
    "                                                   'WELLNUM']].sample(2000), hue='WELLNUM')\n",
    "# data are not stationary, distributions vary with wells, indicating different zones or regional nonstationarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Zonation Based on GR, Calculate Pure Shale Non-Shale GR values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate gr of pure shale and non-shale and perform zonation based on baselines\n",
    "mylist=['GR','VSH']\n",
    "for i in range(len(per_well)):\n",
    "    per_well[i]['sand']=np.nan\n",
    "    per_well[i]['shale']=np.nan\n",
    "    # interpolate when vsh is larger than 0.95 or smaller than 0.05\n",
    "    good0=per_well[i][mylist].notnull().all(1)\n",
    "    good=per_well[i][mylist].notnull().all(1)*(per_well[i]['VSH']<0.95)*(per_well[i]['VSH']>0.05)\n",
    "    data=per_well[i][good].copy()\n",
    "    idx=np.array(data.index)\n",
    "    print(i)\n",
    "    for j in range(data.shape[0]-11):\n",
    "        model= LinearRegression().fit(data['VSH'].values[j:j+11].reshape([-1,1]), data['GR'].values[j:j+11].reshape([-1,1]))\n",
    "        data.loc[idx[j+5],'shale']=model.predict(np.array([1]).reshape(-1,1))[0][0]\n",
    "        data.loc[idx[j+5],'sand']=model.predict(np.array([0]).reshape(-1,1))[0][0]\n",
    "    per_well[i].loc[good,['sand','shale']]=data.loc[:,['sand','shale']].values\n",
    "    if i==1: # results at this interval is not reasonable, interpolation is performed\n",
    "        per_well[i].loc[60766:61017,['sand','shale']]=np.nan # get unreasonable results\n",
    "    per_well[i].loc[good0,['sand','shale']]=per_well[i].loc[good0,['sand','shale']].interpolate(method='nearest', axis=0).ffill().bfill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check variation of pure shale and sand\n",
    "for i in range(len(per_well)):\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.plot(per_well[i]['GR'],label='GR')\n",
    "    plt.plot(per_well[i]['shale'],label='Sh BL')        \n",
    "    plt.plot(per_well[i]['sand'],label='SS BL')  \n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data is noisy, mannualy decide bounaries can improve results\n",
    "good=[]\n",
    "for i in range(len(per_well)):\n",
    "    good.append(~per_well[i][mylist].notnull().all(1))\n",
    "\n",
    "per_well[0]['zone']=0\n",
    "per_well[1]['zone']=1\n",
    "per_well[1].loc[60895:61430,'zone']=2\n",
    "per_well[1].loc[61430:,'zone']=3\n",
    "per_well[2]['zone']=4\n",
    "per_well[2].loc[96440:96500,'zone']=5\n",
    "per_well[2].loc[96500:,'zone']=6\n",
    "per_well[2].loc[97145:97580,'zone']=7\n",
    "per_well[3]['zone']=8\n",
    "per_well[3].loc[129468:130055,'zone']=9\n",
    "per_well[3].loc[130055:,'zone']=10\n",
    "per_well[4]['zone']=11\n",
    "per_well[5]['zone']=12\n",
    "per_well[5].loc[204762:205833,'zone']=13\n",
    "per_well[5].loc[205833:,'zone']=14\n",
    "per_well[6]['zone']=15\n",
    "per_well[7]['zone']=16\n",
    "per_well[7].loc[294410:295450,'zone']=17\n",
    "per_well[7].loc[295450:,'zone']=18\n",
    "per_well[8]['zone']=19\n",
    "per_well[8].loc[314300:,'zone']=20\n",
    "n_zone=21\n",
    "\n",
    "for i in range(len(per_well)):\n",
    "    per_well[i].loc[good[i],'zone']=np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean baselines\n",
    "sand_zone=np.zeros(n_zone)\n",
    "shale_zone=np.zeros(n_zone)\n",
    "zone_size=np.zeros(n_zone)\n",
    "all_train=pd.concat([per_well[i] for i in range(9)],axis=0)\n",
    "for i in range(n_zone):\n",
    "    sand_zone[i]=np.median(all_train.loc[all_train['zone']==i,'sand'].values)\n",
    "    shale_zone[i]=np.median(all_train.loc[all_train['zone']==i,'shale'].values)\n",
    "    zone_size[i]=np.sum(all_train['zone'].values==i)\n",
    "    all_train.loc[all_train['zone']==i,'sand']=sand_zone[i]\n",
    "    all_train.loc[all_train['zone']==i,'shale']=shale_zone[i]\n",
    "\n",
    "per_well=[all_train[all_train['WELLNUM']==i] for i in wells]# update cleaned sand shale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adapt Test Well to Training Well with DTW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thr=200 # highest GR threshould, obtained from the training data, avoid outlier effects\n",
    "# Perform DTW to decide well correlation\n",
    "error=np.zeros((4,9))\n",
    "for i in range(4):\n",
    "    print(i)\n",
    "    for j in range(9):\n",
    "        x=per_well2[i]['GR'].values\n",
    "        x[x>thr]=thr\n",
    "        train=per_well[j].dropna(axis=0,subset=['GR']).copy()\n",
    "        train.loc[train['GR']>thr,'GR']=thr\n",
    "        dtw_1=dtw(x,train['GR'].values,step_pattern='asymmetric',keep_internals=False,open_end=True,open_begin=True)\n",
    "        error[i,j]=dtw_1.normalizedDistance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DTW results\n",
    "# visualize the matched interval\n",
    "train_well_id=[np.argmin(error[i]) for i in np.arange(4)]\n",
    "for i in range(4):\n",
    "    j=train_well_id[i]\n",
    "    x=per_well2[i]['GR'].values\n",
    "    x[x>thr]=thr\n",
    "    train=per_well[j].dropna(axis=0,subset=['GR']).copy()\n",
    "    train.loc[train['GR']>thr,'GR']=thr\n",
    "    dtw_1=dtw(x,train['GR'].values,step_pattern='asymmetric',keep_internals=False,open_end=True,open_begin=True)   \n",
    "    plt.figure()\n",
    "    plt.plot(x[dtw_1.index1s])\n",
    "    plt.plot(train['GR'].values[dtw_1.index2s])\n",
    "    plt.plot(train['shale'].values[dtw_1.index2s])\n",
    "    plt.plot(train['sand'].values[dtw_1.index2s])\n",
    "print(train_well_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which features are useful for baseline calculation for test wells\n",
    "plt.figure()\n",
    "plt.scatter(all_train['GR'],all_train['sand'])\n",
    "plt.xlim(0,40)\n",
    "plt.title('Sand vs GR')\n",
    "plt.figure()\n",
    "plt.scatter(all_train['GR'],all_train['shale'])\n",
    "plt.title('Shale vs GR')\n",
    "# A strong relationship between sand baseline and lower percentiles of GR is observed, \n",
    "# while the relationship to shale based line is not clear.\n",
    "# We decide to use the shale base line from adapted training well and build a model between GR low percentile and sand baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use type-well method described by Pan et al.(2022) to decide training data for each well\n",
    "train_well_id=[np.argmin(error[i]) for i in np.arange(4)]\n",
    "\n",
    "# sand baseline prediction with KNN and moving window, linear model is not used because of normalization\n",
    "gr_sand_train=[i.dropna(axis=0,subset=['GR','VSH','sand']).copy() for i in per_well]\n",
    "features=[]\n",
    "sand_val=[]\n",
    "window_size=1501 # moving window used to decide sand base line, this equals to half of the smallest sand zone observed in training data\n",
    "percentiles=[0,5] # use 0 and 5 percentile, 0 percentile has strongest correlation, and 5 percentile is robust for sand baseline prediction\n",
    "for i in gr_sand_train:\n",
    "    for j in range(i.shape[0]-window_size):\n",
    "        ff=np.percentile(i['GR'].values[j:j+window_size],percentiles)\n",
    "        features.append(ff.copy())\n",
    "        sand_val.append(i['sand'].values[j+window_size//2]-ff[0])\n",
    "\n",
    "features=np.stack(features,axis=0)\n",
    "sand_val=np.stack(sand_val,axis=0)\n",
    "\n",
    "reg = KNeighborsRegressor().fit(features, sand_val) # We use KNN because of well log normalization issues.\n",
    "\n",
    "\n",
    "sand_val=[]\n",
    "for i in per_well2:\n",
    "    features_test=[]\n",
    "    pad_gr=np.concatenate([i['GR'].values[::-1],i['GR'].values,i['GR'].values[::-1]])\n",
    "    for j in np.arange(i.shape[0]-window_size//2,2*i.shape[0]-window_size//2):\n",
    "        features_test.append(np.percentile(pad_gr[j:j+window_size],percentiles))\n",
    "    features_test=np.stack(features_test,axis=0)   \n",
    "    sand_test=reg.predict(features_test)\n",
    "    idx=np.array(i.index)\n",
    "    i['sand_pred']=median_filter(sand_test.flatten()+features_test[:,0].flatten(),window_size)\n",
    "\n",
    "# shale baseline calculation, using the shale baseline of the type well, assuming continuous shale\n",
    "for i in range(len(per_well2)):\n",
    "    j=train_well_id[i]\n",
    "    x=per_well2[i]['GR'].values\n",
    "    x[x>thr]=thr\n",
    "    train=per_well[j].dropna(axis=0,subset=['GR']).copy()\n",
    "    train.loc[train['GR']>thr,'GR']=thr\n",
    "    dtw_1=dtw(x,train['GR'].values,step_pattern='asymmetric',keep_internals=False,open_end=True,open_begin=True)   \n",
    "    shale=train['shale'].values[dtw_1.index2s]\n",
    "    per_well2[i]['shale_pred']=shale[np.isfinite(shale)].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict VSH\n",
    "for i in range(len(per_well2)):\n",
    "    per_well2[i]['vsh_pred']=(per_well2[i]['GR'].values-per_well2[i]['sand_pred'].values)/(per_well2[i]['shale_pred'].values-per_well2[i]['sand_pred'].values)\n",
    "    per_well2[i].loc[per_well2[i]['vsh_pred']<0,'vsh_pred']=0\n",
    "    per_well2[i].loc[per_well2[i]['vsh_pred']>1,'vsh_pred']=1    \n",
    "\n",
    "\n",
    "all_test2=pd.concat(per_well2,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict  Porosity\n",
    "- Since porosity estimation error is low in the validation process, we didn't spend much time and used simple linear model to perform the calculation \n",
    "- Test wells are adapted to its type well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train=pd.concat(per_well,axis=0)\n",
    "\n",
    "\n",
    "for i in range(len(per_well2)):\n",
    "    por_thr=0.05 # found truncations at 0.02 and 0, thus set a threshould for a better linear model training\n",
    "    poro_train=per_well[train_well_id[i]].dropna(axis=0,subset=['DEN','PHIF']).copy() # a strong linear relationship exist between PHIF and DEN, for simplicity we only use DEN to predict PHIF\n",
    "    x=poro_train['DEN'].values\n",
    "    y=poro_train['PHIF'].values\n",
    "    x=x[x>por_thr]\n",
    "    y=y[x>por_thr]\n",
    "    model= LinearRegression().fit(x.reshape([-1,1]),y.reshape([-1,1]))    \n",
    "    \n",
    "    per_well2[i]['poro_pred']=model.predict(per_well2[i]['DEN'].values.reshape(-1,1)).flatten()#yy2.copy()\n",
    "    per_well2[i].loc[per_well2[i]['poro_pred']<0,'poro_pred']=0\n",
    "    \n",
    "all_test2=pd.concat(per_well2,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SW  Prediction\n",
    "- Previous adaption didn't consider fluid effects, thus we use method proposed by Pan et al.(2022) to calculate new type wells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include porosity logs and resistivity logs\n",
    "mykeys=['DEN','NEU','log_rd','SW']\n",
    "mykeys0=['DEN','NEU','RDEP','SW']\n",
    "per_well=[df1[df1['WELLNUM']==i].copy() for i in wells]   \n",
    "# transform resistivity to logorithmic scale\n",
    "for i in range(len(per_well)):\n",
    "    per_well[i]=per_well[i].dropna(axis=0,subset=mykeys0).copy()\n",
    "    per_well[i]['depth_norm']=(per_well[i]['DEPTH'].values-per_well[i]['DEPTH'].min())/(per_well[i]['DEPTH'].max()-per_well[i]['DEPTH'].min())\n",
    "    per_well[i]['log_rd']=np.log(per_well[i]['RDEP'].values)\n",
    "    per_well[i].dropna(axis=0, subset=mykeys, inplace=True)\n",
    "for i in range(len(per_well2)):\n",
    "    per_well2[i]['depth_norm']=(per_well2[i]['DEPTH'].values-per_well2[i]['DEPTH'].min())/(per_well2[i]['DEPTH'].max()-per_well2[i]['DEPTH'].min())\n",
    "    per_well2[i]['log_rd']=np.log(per_well2[i]['RDEP'].values)\n",
    "\n",
    "\n",
    "well_list=np.arange(9)\n",
    "all_train=pd.concat([per_well[i] for i in well_list],axis=0).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# statistical distance calculation\n",
    "def KLdivergence(x, y):\n",
    "  \"\"\"Compute the Kullback-Leibler divergence between two multivariate samples.\n",
    "  Parameters\n",
    "  ----------\n",
    "  x : 2D array (n,d)\n",
    "    Samples from distribution P, which typically represents the true\n",
    "    distribution.\n",
    "  y : 2D array (m,d)\n",
    "    Samples from distribution Q, which typically represents the approximate\n",
    "    distribution.\n",
    "  Returns\n",
    "  -------\n",
    "  out : float\n",
    "    The estimated Kullback-Leibler divergence D(P||Q).\n",
    "  References\n",
    "  ----------\n",
    "  Pérez-Cruz, F. Kullback-Leibler divergence estimation of\n",
    "continuous distributions IEEE International Symposium on Information\n",
    "Theory, 2008.\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "  # Check the dimensions are consistent\n",
    "  x = np.atleast_2d(x)\n",
    "  y = np.atleast_2d(y)\n",
    "\n",
    "  n,d = x.shape\n",
    "  m,dy = y.shape\n",
    "\n",
    "  assert(d == dy)\n",
    "\n",
    "\n",
    "  # Build a KD tree representation of the samples and find the nearest neighbour\n",
    "  # of each point in x.\n",
    "  xtree = KDTree(x)\n",
    "  ytree = KDTree(y)\n",
    "\n",
    "  # Get the first two nearest neighbours for x, since the closest one is the\n",
    "  # sample itself.\n",
    "  r = xtree.query(x, k=2, eps=.01, p=2)[0][:,1]\n",
    "  s = ytree.query(x, k=1, eps=.01, p=2)[0]\n",
    "\n",
    "  # There is a mistake in the paper. In Eq. 14, the right side misses a negative sign\n",
    "  # on the first term of the right hand side.\n",
    "  return -np.log(r/s).sum() * d / n + np.log(m / (n - 1.))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate type well based on KL div\n",
    "\n",
    "all_well=per_well+per_well2\n",
    "list1=['DEN','NEU','log_rd']\n",
    "all_well=[i.dropna(axis=0, subset=list1) for i in all_well]\n",
    "\n",
    "thr=np.inf # no thr for GR, GR is not used\n",
    "\n",
    "div_dist=np.zeros([len(all_well),len(all_well)])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(median_filter(all_train[list1].values,size=(7,1),mode='nearest')) # remove outliers and noises\n",
    "\n",
    "\n",
    "for ii in range(len(all_well)):\n",
    "    m=all_well[ii][list1].values[all_well[ii]['GR'].values<thr]\n",
    "    m=median_filter(m,size=(7,1),mode='nearest') # remove outlier\n",
    "    x=scaler.transform(m)\n",
    "\n",
    "    for i in range(len(all_well)):\n",
    "        if i<ii and ii>0:\n",
    "            m2=all_well[i][list1].values[all_well[i]['GR'].values<thr]\n",
    "            m2=median_filter(m2,size=(7,1),mode='nearest')\n",
    "            y=scaler.transform(m2)#.reshape([-1,1])\n",
    "            x=x+0.0000001*np.random.normal(0,1,x.shape) # well logs are rounded off,small values do not change the result\n",
    "            y=y+0.0000001*np.random.normal(0,1,y.shape)\n",
    "            div_dist[ii,i]=KLdivergence(x,y)+KLdivergence(y,x)\n",
    "            \n",
    "for ii in range(len(all_well)):\n",
    "    for i in range(len(all_well)):\n",
    "        if i>ii:\n",
    "            div_dist[ii,i]=div_dist[i,ii]\n",
    "\n",
    "\n",
    "train_well_id=[np.argsort(div_dist[i+9,:9])[0] for i in np.arange(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "well_list=np.arange(9)\n",
    "for i in range(4):\n",
    "    well_list=[train_well_id[i]]\n",
    "    all_train=pd.concat([per_well[i] for i in well_list],axis=0)\n",
    "    \n",
    "    # simple model\n",
    "    clf=KNeighborsRegressor(n_neighbors=50)\n",
    "\n",
    "    x_dat=scaler.transform(all_train[list1].values)\n",
    "    \n",
    "    y_dat=all_train['SW'].values\n",
    "    \n",
    "    clf.fit(x_dat,y_dat)\n",
    "    \n",
    "    x_dat2=scaler.transform(per_well2[i][list1].values)\n",
    "    y_pred_dat=clf.predict(x_dat2)\n",
    "\n",
    "    per_well2[i]['Sw2']=y_pred_dat\n",
    "    per_well2[i].loc[per_well2[i]['Sw2']>1,'Sw2']=1\n",
    "    per_well2[i].loc[per_well2[i]['Sw2']<0,'Sw2']=0\n",
    "\n",
    "all_test2=pd.concat(per_well2,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Apply semi-supervised learning to correct imputed data, and incorporate spatial continuity with truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing data are in a continuous interval of well 1\n",
    "# Use accurate predictions in the same well to calculate SW at imputed points in well 1\n",
    "# spatial continuity of saturation can be used to constrain the prediction\n",
    "# correct imputed interval predicton with semisupervised learning and local data\n",
    "df22 = pd.read_csv('test.csv')\n",
    "df22.replace(['-9999', -9999], np.nan, inplace=True)\n",
    "wells2=df22['WELLNUM'].unique()\n",
    "per_well22=[df22[df22['WELLNUM']==i] for i in wells2]  \n",
    "locs=np.where(np.isnan(df22['RDEP']))[0]\n",
    "locs2=np.where(np.isfinite(per_well22[1]['RDEP']))[0]\n",
    "\n",
    "# choose nearby intervals for training\n",
    "# gr and neu are available\n",
    "scaler = StandardScaler()\n",
    "x_train=scaler.fit_transform(per_well22[1].loc[np.array(per_well2[1].index)[locs2[:800]],['GR','NEU']]) # use spatially close results\n",
    "x_test=scaler.transform(per_well22[1].loc[locs,['GR','NEU']])\n",
    "y_train=per_well2[1].loc[np.array(per_well2[1].index)[locs2[:800]],'Sw2']\n",
    "clf=KNeighborsRegressor(n_neighbors=100)\n",
    "clf.fit(x_train,y_train)\n",
    "y_impute_pred=clf.predict(x_test)\n",
    "per_well2[1].loc[locs,'Sw2']=y_impute_pred\n",
    "all_test2=pd.concat(per_well2,axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd=pd.read_csv('submission.csv')\n",
    "dd.keys()\n",
    "dd2=dd.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd['PHIF']=all_test2['poro_pred'].values\n",
    "dd['SW']=all_test2['Sw2'].values\n",
    "dd['VSH']=all_test2['vsh_pred'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sqrt(np.mean((dd.values-dd2.values)**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.to_csv('submission_final.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize results\n",
    "for c,i in enumerate(per_well2):\n",
    "    for j in ['poro_pred','Sw2','vsh_pred']:\n",
    "        plt.figure(figsize=(12,3))\n",
    "        plt.plot(i[j])\n",
    "        plt.title(j+'_well'+str(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a>7. Discussion</a>\n",
    "\n",
    "There are many things worth further exploration. We list several of them here:\n",
    "1. Using other methods to find type well\n",
    "2. Try multiple type wells for a test well\n",
    "3. Apply more sophisticated models to make robust prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Giorgino, T., Computing and Visualizing Dynamic Time Warping Alignments in R: The dtw Package.\n",
    "\n",
    "Pan, W., Verdin, C., Duncan, I., Pyrcz, M., 2022, Reducing the Uncertainty of Multi-Well Petrophysical Interpretation from Well Logs with Statistical and Machine Learning Models, submitted\n",
    "\n",
    "Pan, W., Verdin, C., Duncan, I., Pyrcz, M.,2021, Feature Engineering in Well-Log Interpretation, presented at GEOGULF 2021\n",
    "\n",
    "Pan, W., Verdin, C., Duncan, I., Pyrcz, M., 2021, Well-Log Normalization and Well-to-Well Correlation via Machine Learning Methods presented at 2021 UT DIRECT and Formation Evaluation Consortium\n",
    "\n",
    "Pérez-Cruz, F. Kullback-Leibler divergence estimation ofcontinuous distributions IEEE International Symposium on Information Theory, 2008.\n",
    "\n",
    "J. Stat. Soft., doi:10.18637/jss.v031.i07\n",
    "\n",
    "Yu, Y., Xu, C., Misra, S., Li, W., Ashby, M., Pan, W., Deng, T., Jo, H., Santos, J.E., Fu, L. and Wang, C., 2021, Synthetic Sonic Log Generation With Machine Learning: A Contest Summary From Five Methods, Petrophysics, 62(4), 393–406.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdda",
   "language": "python",
   "name": "pdda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
